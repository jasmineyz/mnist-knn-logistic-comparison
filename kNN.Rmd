---
title: "kNN Analysis"
author: "Jasmine Zhang"
date: "2025-03-19"
output: html_document
---

```{r setup, include=FALSE}
# Load required libraries
library(tidyverse)
library(caret)  # Ensure caret is loaded
library(dslabs)
library(gridExtra)

# Load dataset
data("mnist_27")
```

# Introduction

In this analysis, we compare **k-Nearest Neighbors (kNN) classification** with **logistic regression** using the `mnist_27` dataset. Our goal is to evaluate how different values of `k` impact model performance and how kNN compares to logistic regression in terms of accuracy and decision boundaries.

Letâ€™s start by loading the data and showing a plot of the predictors with outcome represented with color.

```{r}
mnist_27$test |> ggplot(aes(x_1, x_2, color = y)) +  geom_point()
```


# Training and Evaluating kNN (k = 5)

First, we train a kNN model with **k = 5**, a common choice that balances bias and variance. We then evaluate its accuracy on the test set.

```{r}
knn_fit <- knn3(y ~ ., data = mnist_27$train)

y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
kNN_5_accuracy <- confusionMatrix(y_hat_knn, mnist_27$test$y)$overall["Accuracy"]
kNN_5_accuracy
```

# Training and Evaluating Logistic Regression

Next, we fit a **logistic regression model**. Since logistic regression provides linear decision boundaries, it serves as a useful benchmark against kNN.

```{r}
fit_lm <- mnist_27$train %>% 
  mutate(y = ifelse(y == 7, 1, 0)) %>%
  lm(y ~ x_1 + x_2, data = .)

p_hat_lm <- predict(fit_lm, mnist_27$test)
y_hat_lm <- factor(ifelse(p_hat_lm > 0.5, 7, 2))
logistic_accuracy <- confusionMatrix(y_hat_lm, mnist_27$test$y)$overall["Accuracy"]
logistic_accuracy
```

# Visualizing Conditional Probabilities

To better understand model performance, we visualize the true conditional probabilities and compare them with the decision boundary estimated by kNN with k = 5.

```{r, warning=FALSE}
plot_cond_prob <- function(p_hat=NULL){
  tmp <- mnist_27$true_p
  if(!is.null(p_hat)){
    tmp <- mutate(tmp, p=p_hat)
  }
  tmp %>% ggplot(aes(x_1, x_2, z=p, fill=p)) +
    geom_raster(show.legend = FALSE) +
    scale_fill_gradientn(colors=c("#F8766D", "white", "#00BFC4")) +
    stat_contour(breaks=c(0.5), color="black")
}

p1 <- plot_cond_prob() + ggtitle("True conditional probability")
p2 <- plot_cond_prob(predict(knn_fit, mnist_27$true_p)[,2]) + ggtitle("kNN-5 estimate")
grid.arrange(p2, p1, nrow=1)
```

We see that kNN better adapts to the non-linear shape of $p(x_1, x_2)$. However, our estimate has some islands of blue in the red area, which intuitively does not make much sense. This is due to *over-training*.

# Overtraining: kNN with k = 1

Now, we reduce `k` to **1**, which means each prediction is based on the closest neighbor. This results in high variance and likely overfitting.

```{r}
knn_fit_1 <- knn3(y ~ ., data = mnist_27$train, k = 1)
y_hat_knn_1 <- predict(knn_fit_1, mnist_27$test, type = "class")
kNN_1_accuracy <- confusionMatrix(y_hat_knn_1, mnist_27$test$y)$overall["Accuracy"]
kNN_1_accuracy
```

# Oversmoothing: kNN with k = 401

Next, we set `k` to **401**, meaning the decision boundary is based on a large number of neighbors, leading to oversmoothing.

```{r}
knn_fit_401 <- knn3(y ~ ., data = mnist_27$train, k = 401)
y_hat_knn_401 <- predict(knn_fit_401, mnist_27$test, type = "class")
kNN_401_accuracy <- confusionMatrix(y_hat_knn_401, mnist_27$test$y)$overall["Accuracy"]
kNN_401_accuracy
```
While this approach prevents overfitting, it reduces model flexibility and may perform worse on complex patterns.

# Comparing Logistic Regression vs. kNN-401

Finally, we compare the decision boundaries of **logistic regression** and **kNN with k = 401** to see how they differ in classification behavior.

```{r, warning=FALSE}
fit_glm <- glm(y ~ x_1 + x_2, data=mnist_27$train, family="binomial")

p1 <- plot_cond_prob(predict(fit_glm, mnist_27$true_p)) + ggtitle("Logistic Regression")
p2 <- plot_cond_prob(predict(knn_fit_401, mnist_27$true_p)[,2]) + ggtitle("kNN-401")
grid.arrange(p1, p2, nrow=1)
```

This size of $k$ is so large that it does not permit enough flexibility
